# LLM Playground with R and ellmer

This notebook demonstrates how to interact with Large Language Models using the `ellmer` package in R. We'll explore text generation, structured data extraction, and various AI capabilities.

## Setup

You'll need to set up your Groq API key. Get one from https://console.groq.com/keys, and then set it in your .Renviron file using `usethis::edit_r_environ()`. Restart R for it to take effect.

Then, let's install and load the necessary packages:

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r install-packages, eval=FALSE}
# Install required packages
install.packages(c("ellmer", "httr", "jsonlite", "rvest"))
```

```{r load-packages}
library(tidyverse)
library(lubridate)
library(ellmer)
library(httr)
library(jsonlite)
library(rvest)
```

### Testing Out Groq Using Llama 4

We'll use one of Groq's recent additions, the Llama 4 Scout model from Meta. In ellmer, you construct a chat object like this:

```{r groq-setup}

# Set the model
chat <- chat_groq(
  model = "meta-llama/llama-4-scout-17b-16e-instruct"
)

chat$chat("Give me 20 names for a pet turtle")

```

## Text Models and Summarization

Let's start by fetching web content and summarizing it using an LLM. We need to add some custom headers to mimic a browser because Nieman Lab doesn't like folks scraping their content.

```{r web-scraping}
# Function to fetch and clean web content
fetch_article <- function(url) {
  tryCatch({
    page <- read_html(GET(url, 
        add_headers(
      "User-Agent" = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
      "Accept" = "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
      "Accept-Language" = "en-US,en;q=0.5",
      "Accept-Encoding" = "gzip, deflate",
      "Connection" = "keep-alive",
      "Referer" = "https://www.google.com/"
    ),
      timeout(30)
    ))
    # Extract story content using css selectors
    content <- page |> 
      html_nodes(".simple-leftstream, article, main, .content") |> 
      html_text() |>
      paste(collapse = "\n")
    
    # Clean up whitespace
    content <- gsub("\\s+", " ", content)
    content <- trimws(content)
    
    return(content)
  }, error = function(e) {
    return(paste("Error fetching content:", e$message))
  })
}

# Fetch the Nieman Lab article
article_url <- "https://www.niemanlab.org/2025/02/meet-the-journalists-training-ai-models-for-meta-and-openai/"
article_text <- fetch_article(article_url)
```

```{r summarize-article}

chat$chat(paste(
    "Summarize this story in 3 paragraphs:",
    article_text)
)
```

Compare this summary to the original article. How well did the LLM capture the key points?

## Structured Data Extraction

Ellmer does support the production of structured output (data), but Groq doesn't yet support that, so we're going to do things the old-fashioned way. Let's work with the Maryland attorney sanctions data and convert it to JSON that we can then turn into a dataframe:

```{r load-sanctions-data}
# Read the sanctions text file
sanctions_text <- readLines("sanctionsfy25.txt", warn = FALSE)
sanctions_content <- paste(sanctions_text, collapse = "\n")

# Display first few lines
cat("Sanctions data preview:\n")
cat(paste(head(sanctions_text, 10), collapse = "\n"))
```

```{r extract-structured-data}
# Create prompt for structured data extraction
structured_response <- chat$chat(paste(
  "produce only a list of JSON objects based on the text with the following keys: name, sanction, date, description.",
  "The date should be in the yyyy-mm-dd format. No yapping.",
  "\nText:",
  sanctions_content
))


# Display the response
cat("Structured Data Response:\n")
cat(structured_response)
```

Then we can clean that up a bit and turn it into tidy data:

```{r parse-json-data}
# Parse the JSON response

json_content <- structured_response |>
  gsub("```json\n?", "", .) |>
  gsub("\n?```", "", .) |>
  trimws()

sanctions_df <- fromJSON(json_content) |>
  as_tibble() |>
  mutate(date = ymd(date))

```

**Your Evaluation:** How well did the LLM perform on this data extraction task? How should we evaluate it?

## Vision Models (Image Analysis)

Groq currently supports a few multimodal models, including the two from Llama 4 models from Meta. You can pass local images or image URLs to it using the `content_image_file()` and `content_image_url()` functions from ellmer. Take a look at the `md_doc.png` file in this repository.

```{r vision-setup}

image <- content_image_file("md_doc.png")

license_number <- chat$chat("what is the license number from this image?", image)

```


```{r}

pdf <- content_image_file("BlackPop1930.pdf")

extract_csv <- chat$chat("what is the license number from this image?", image)

```



### Your Turn: Find an image from the web and ask Llama 4 a question about it using `content_image_url()`


```{r}


```

How well did the vision model perform on your image?


### Audio Models

Another option is to use models that transcribe audio. Download and listen to [this mp3 file](https://dare.wisc.edu/audio/new-mexico-chuck-wagon-etiquette/) from the Dictionary of American Regional English project at the University of Wisconsin. Then, in Groq's dev console, change the model to `distil-whisper-large-v3-en` and upload the mp3 file using the "Select File" button. Then hit submit and check out the transcript. How did the LLM do compared to the original transcript?