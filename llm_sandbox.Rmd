# LLM Playground with R and ellmer

This notebook demonstrates how to interact with Large Language Models using the `ellmer` package in R. We'll explore text generation, structured data extraction, and various AI capabilities.

## Setup

You'll need to set up your Groq API key. Get one from https://console.groq.com/keys, and then set it in your .Renviron file using `usethis::edit_r_environ()`. Restart R for it to take effect.

Then, let's install and load the necessary packages:

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r install-packages, eval=FALSE}
# Install required packages
install.packages(c("ellmer", "httr", "jsonlite", "rvest"))
```

```{r load-packages}
library(tidyverse)
library(lubridate)
library(ellmer)
library(httr)
library(jsonlite)
library(rvest)
```

### Testing Out Groq Using Llama 4

We'll use one of Groq's recent additions, the Llama 4 Scout model from Meta. In ellmer, you construct a chat object like this:

```{r groq-setup}

# Set the model
chat <- chat_groq(
  model = "meta-llama/llama-4-scout-17b-16e-instruct",
)

chat$chat("Give me 20 names for a pet turtle")

```

## Text Models and Summarization

Let's start with this [Nieman Lab piece](https://www.niemanlab.org/2025/02/meet-the-journalists-training-ai-models-for-meta-and-openai/) - read that. Then we'll load the text:

```{r read-article}
article_text <- readLines("nieman_lab.txt", warn = FALSE)
```

And then we'll have Llama 4 summarize it:

```{r summarize-article}

chat$chat(paste(
    "Summarize this story in 3 paragraphs:",
    article_text)
)
```

Compare this summary to the original article. How well did the LLM capture the key points?

## Structured Data Extraction

Ellmer does support the production of structured output (data), but Groq doesn't yet, so we're going to do things the old-fashioned way. Let's work with some descriptions of Maryland attorney sanctions and convert it to JSON that we can then turn into a dataframe:

```{r load-sanctions-data}
# Read the sanctions text file
sanctions_text <- readLines("sanctionsfy25.txt", warn = FALSE)
sanctions_content <- paste(sanctions_text, collapse = "\n")

# Display first few lines
cat("Sanctions data preview:\n")
cat(paste(head(sanctions_text, 10), collapse = "\n"))
```

```{r extract-structured-data}
# Create prompt for structured data extraction
structured_response <- chat$chat(paste(
  "produce only a list of JSON objects based on the supplied text with the following keys: name, sanction, date, description.",
  "The date should be in the yyyy-mm-dd format. Do not include any introductory text, no yapping.",
  "\nText:",
  sanctions_content
))


# Display the response
cat("Structured Data Response:\n")
cat(structured_response)
```

Then we can turn it into tidy data:

```{r parse-json-data}
# Parse the JSON response

sanctions_df <- fromJSON(structured_response) |>
  as_tibble() |>
  mutate(date = ymd(date))

```

**Your Evaluation:** How well did the LLM perform on this data extraction task? How should we evaluate it?

## Vision Models (Image Analysis)

Groq currently supports a few multimodal models, including the two from Llama 4 models from Meta. You can pass local images or image URLs to it using the `content_image_file()` and `content_image_url()` functions from ellmer. Take a look at the `md_doc.png` file in this repository.

```{r vision-setup}

image <- content_image_file("md_doc.png")

license_number <- chat$chat("what is the license number from this image?", image)

```

Text extraction is hard, and for complicated jobs, you definitely need to check the results. We'll use an image of Black population figures by city from 1930. Check those results and compare them to the image in this repository.

```{r}

black_pop <- content_image_url("https://raw.githubusercontent.com/dwillis/llm-djnf25/refs/heads/main/BlackPop1930.png")

extract_csv <- chat$chat("Extract the information in this image into a CSV file, preserving the rows and columns and ensuring accuracy", black_pop)

```


### Your Turn: Find an image from the web and ask Llama 4 a question about it using `content_image_url()`


```{r}


```

How well did the vision model perform on your image?


### Audio Models

Another option is to use models that transcribe audio. Download and listen to [this mp3 file](https://dare.wisc.edu/audio/new-mexico-chuck-wagon-etiquette/) from the Dictionary of American Regional English project at the University of Wisconsin. Then, in Groq's dev console, change the model to `distil-whisper-large-v3-en` and upload the mp3 file using the "Select File" button. Then hit submit and check out the transcript. How did the LLM do compared to the original transcript?